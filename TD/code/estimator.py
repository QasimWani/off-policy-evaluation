### define Importance Sampling Estimator functions
from model import Agent

class IS():
    """Class to implement different Importance Sampling functions"""
    def __init__(self, env, type="OIS"):
        """Define Importance Sampling function type"""
        self.env = env
        self.func = self.RIS if(type == "RIS") else self.OIS
        self.D = None #build dataset for computing π_d
        
    def OIS(self, evaluation_agent:Agent, behavior_agent:Agent):
        """
        > Calculates ordinary importance sampling between behavior agent and evaluation agent
        > Calculates Return value (total reward) for behavior_agent
        NOTE: behavior_agent (π_k) generates all states and action.
        @Param:
        1. evaluation_agent - agent class object representing evaluation policy.
        2. behavior_agent - agent class object representing behavior policy.
        @Returns:
        - ois : importance sampling ratio ratio b/w evaluation and behavior agent.
        - total_reward : return of behavior_agent.
        """
        ois = 1
        total_reward = 0
        state = self.env.reset() #reset

        counter = 0 #counter for maximum limit

        while True:
            action, prob_behv = behavior_agent.get_action(behavior_agent.Q, state, eps=0) #generate best action and prob for behavior.
            _, prob_eval = evaluation_agent.get_action(evaluation_agent.Q, state, eps=0) #generate max probability for evaluation policy.
            ois *= float(prob_eval/prob_behv) #compute IS
            next_state, reward, done, info = self.env.step(action) #transition

            total_reward += reward #update reward
            state = next_state #update state
            counter += 1
            
            if(done or counter > 125): #stopping condition
                break

        return ois, total_reward
    
    def RIS(self, evaluation_agent, behavior_agent):
        """
        > Calculates Regression Importance Sampling ratio between evaluation and behavior policy.
        > Calculates Return (total reward) for behavior_agent
        NOTE: behavior_agent (π_k) generates all states and action.
        @Param:
        1. evaluation_agent - agent class object representing evaluation policy.
        2. behavior_agent - agent class object representing behavior policy.
        @Returns:
        - ris : regression importance sampling ratio ratio b/w evaluation and behavior agent.
        - total_reward : return of behavior_agent.
        """
        #Steps: 
        # 1. generate feature matrix, D, conditioned on π_b DONE
        # 2. count(h, a) and count(h) accordingly. 
        # 3. generate the estimate for π_b as π_d
        # 4. compute the RIS ratio and return value
        ris = 1
        D, reward = self.gen_feat_matrix(behavior_agent)
        self.D = D #set dataset

        for state, action in zip(*D):
            _, prob_eval = evaluation_agent.get_action(evaluation_agent.Q, state, eps=0) #generate max probability for evaluation policy.
            prob_estimate_behv = self.count(state, action)
            ris *= float(prob_eval/prob_estimate_behv) #compute RIS
        return ris, reward
    
    def count(self, observation, action):
        """Compute count-based estimate for π_d(a|h_{i - n:i})"""
        if(self.D is None):
            raise ValueError("Dataset cannot be empty!")

        count_action, count_observation = 0, 0 #define count(h, a) and count(h) respectively
        for s, a in zip(*self.D):
            if(observation == s and action == a):#update count(h, a)
                count_action += 1
                
            if(observation == s): #update count(h)
                count_observation += 1
        
        return count_action / count_observation
                
            
        
    def gen_feat_matrix(self, agent):
        """Generate data D and associated return conditioned on behavior policy π_b"""
        if(self.func != self.RIS):
            raise ValueError("gen_feat_matrix requires using RIS as sampling function")
        
        S = [] #set of States in Dataset, D
        A = [] #set of actions conditioned on S^{n - 1} in dataset, D generated by π_b
        
        state = self.env.reset()
        
        counter = 0
        total_reward = 0
        while True:
            action, _ = agent.get_action(agent.Q, state, eps=0) #generate best action and prob for behavior.
            next_state, reward, done, info = self.env.step(action) #transition

            #### Update ####
            counter += 1
            S.append(state)
            A.append(action)
            #### Update ####
            
            total_reward += reward #update reward
            state = next_state #update state        
            
            if(done or counter > 125): #stopping condition
                break
        return (S, A), total_reward