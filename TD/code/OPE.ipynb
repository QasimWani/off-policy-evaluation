{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy Policy Evaluation on a discrete MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from os import listdir, getcwd\n",
    "from os.path import isabs, join, isfile\n",
    "import re\n",
    "\n",
    "from model import Agent\n",
    "import utils\n",
    "from estimator import IS\n",
    "from wrapper import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create gym environment\n",
    "env_name = \"Taxi-v3\"\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped #gets ride of TimeLimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env) #create TD agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policies(path):\n",
    "    \"\"\"Loads all policies in a directory\"\"\"\n",
    "    cwd = getcwd() #get current working directory\n",
    "    index = lambda idx : int(re.findall(r'[0-9]+', idx)[0]) #get policy number\n",
    "    def model(f):\n",
    "        policy = utils.load_policy(f) if \".npy\" in f else None #load agent\n",
    "        if(policy is None):\n",
    "            return None\n",
    "        agx = Agent(env)\n",
    "        agx.Q = policy\n",
    "        return agx\n",
    "    \n",
    "    return {index(f): model(join(path, f)) for f in listdir(join(cwd, path)) if isabs(join(cwd, f)) and model(join(path, f)) is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = load_policies(\"../model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_agents(indices):\n",
    "    \"\"\"Store selected list of agents given by their keys\"\"\"\n",
    "    agx, idx = [0]*len(indices), [0]*len(indices)\n",
    "    for i, elem in enumerate(indices):\n",
    "        agx[i] = matrix[elem]\n",
    "        idx[i] = elem\n",
    "    return agx, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents, idx = store_agents([4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - generate policy matrix\n",
    "<p> A policy matrix is a dictionary of dimensions (K, K - 1), where K is the total number of behavior policies </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_matrix(agents):\n",
    "    \"\"\"Generates policy matrix (dictionary) of shape (n, n-1) for x agents\"\"\"\n",
    "    matrix = {}\n",
    "    for i, evaluation in enumerate(agents):\n",
    "        matrix[evaluation] = []\n",
    "        for j, behavior in enumerate(agents):\n",
    "            if(i != j):\n",
    "                matrix[evaluation].append(behavior)\n",
    "    return matrix        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each key indicates evaluation policy, and corresponding values indicate behavior policies\n",
    "policy_dict = policy_matrix(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_behavior_policies(evaluation_policy):\n",
    "    \"\"\"Generates respective behvaior policies for a particular evaluation policy\"\"\"\n",
    "    return policy_dict[evaluation_policy] if evaluation_policy in policy_dict.keys() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Importance Sampling\n",
    "\n",
    "$$ \\xi_k =  \\prod_{t=1}^{H} \\frac{ \\pi_e({a_t}^k | {s_t}^k) }{ \\pi_k({a_t}^k | {s_t}^k) } $$\n",
    "\n",
    "### Regression Importance Sampling to approximate for the behavior policy\n",
    "> RIS paper: http://proceedings.mlr.press/v97/hanna19a/hanna19a.pdf\n",
    "\n",
    "$$\\tag{1} \\pi_{D}(a | h_{i - n:i}) := \\frac{c(h_{i - n:i}, a)}{c(h_{i - n:i})} $$\n",
    "- $c(h_{i:j}, a)$ is the count of number of times that action $a$ is observed following trajectory segment $h_{i:j}$ during any trajectory $D$.\n",
    "- $c(h_{i:j})$ is the count of number of times that trajectory segment $h_{i:j}$ appears during any trajectory in $D$.\n",
    "- Note, that we can calculate maximum-likelihood behavior policy with count based estimates\n",
    "\n",
    "Incorporating $(1)$ to compute RIS:\n",
    "$$ RIS{(n)}(\\pi_e, D) := \\frac{1}{m} \\sum\\limits_{i=1}^m g(H_i) \\prod\\limits_{t=0}^{L-1} \\frac{ \\pi_e(A_t|S_t) }{ {\\pi_D}^{(n)}(A_t|H_{t-n:t}) } $$\n",
    "- $g(H_i)$ is the discounted return of trajectory H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_function = IS(env, \"RIS\").func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Get Trajectories\n",
    "\n",
    "$$ \\sigma(\\pi_e, \\pi_k) = \\sum\\limits_{i=1}^N {R_k}^i \\times {\\xi_k}^i  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories(evaluation_agent, behavior_agent, N, type):\n",
    "    \"\"\"\n",
    "    See formula above.\n",
    "    Calculates the sum of value for a behavior policy and corresponding evaluation policy\n",
    "    based on N trajectories.\n",
    "    \n",
    "    > Importance Sampling ratio, ðœ‰\n",
    "    > Compute sigma that represents inner sum for 1 behavior policy and corresponding evaluation policy\n",
    "    \n",
    "    @Param:\n",
    "    1. evaluation_agent - (Agent) Evaluation Policy\n",
    "    2. behavior_agent - (Agent) Behavior Policy\n",
    "    3. N - (int) number of trajectories\n",
    "    4. type - (int) 0/1 representing x_1 (0) and x_2 (1) for matrix multiplication later.\n",
    "    @Return \n",
    "    - sigma = (float) returns inner sum, sigma (see formula for more details).\n",
    "    \"\"\"\n",
    "    sigma = 0\n",
    "    for _ in range(N):\n",
    "        xi, reward = sampling_function(evaluation_agent, behavior_agent)\n",
    "        sigma += reward * xi if(type == 0) else reward #X_2 doesn't use xi because it's distributed later.\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Calculate function value\n",
    "\n",
    "$$ f(\\hat{\\pi}) = \\frac{1}{K N} \\sum\\limits_{k=1}^K \\sigma(\\pi_e, \\pi_k) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(behavior_agents, evaluation_agent, N, type):\n",
    "    \"\"\"\n",
    "    Computes f(Ï€) using formula shown above.\n",
    "    @Param:\n",
    "    1. behavior_agent: (list) list of Agent class objects representing set of evaluation for given Ï€_e.\n",
    "    2. evaluation_agents: Ï€_e (Agent) behavior policy\n",
    "    3. N - (int) number of trajectories (used in calculation of sigma in `get_trajectories`)\n",
    "    4. type: (int) 0/1 for calculation of X_1/X_2 with regards sum of products w.r.t constants, c and d.\n",
    "    @Return\n",
    "    - value: (float) value of function f(Ï€) using formula above.\n",
    "    \"\"\"\n",
    "    value = 0\n",
    "    K = len(behavior_agents)\n",
    "    for agent in behavior_agents:\n",
    "        sigma = get_trajectories(evaluation_agent, agent, N, type)\n",
    "        value += sigma\n",
    "    \n",
    "    return float(value / (K * N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. True Value function for an evaluation agent\n",
    "\n",
    "$$ V(\\pi_e) = \\frac{1}{N} \\sum\\limits_{i=1}^N {R_e}^i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value(policy_dict, N):\n",
    "    \"\"\"\n",
    "    Calculates the expected return for all evaluation agents\n",
    "    @Param: \n",
    "    1. policy_dict - (dict[list]) policy matrix of shape (n, n-1) for n agents.\n",
    "    2. N - (int) number of trajectories to run value estimation for.\n",
    "    NOTE: N should equal with estimation value function parameter N.\n",
    "    @Return:\n",
    "    - values - (nd.array) Vector of values of evaluation policies using formula shown above. \n",
    "    \"\"\"\n",
    "    evaluation_agents = list(policy_dict.keys()) #generate n eval agents\n",
    "    values = [0]*len(evaluation_agents) #vector of values of evaluation policies\n",
    "    for i, agent in enumerate(evaluation_agents):\n",
    "        value = [] #stores N Return for agent with policy Ï€_e.\n",
    "        for n in range(N):\n",
    "            total_reward = 0\n",
    "            state = env.reset() #reset\n",
    "            counter = 0\n",
    "            while True:\n",
    "                action, prob = agent.get_action(agent.Q, state, eps=0)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                counter += 1\n",
    "                if(done or counter > 500): break\n",
    "                    \n",
    "            #append to value\n",
    "            value.append(total_reward)\n",
    "        #compute mean return and store in vector of values.\n",
    "        expected_val = np.mean(value)\n",
    "        values[i] = expected_val\n",
    "    \n",
    "    return np.array([values]).T #dim = number of agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Tying it al together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve an evaluation policy with agent of index i.\n",
    "get_eval_agent = lambda i: list(policy_dict.keys())[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(policy_dict, N):\n",
    "    \"\"\"\n",
    "    Main function for generating X_1 and X_2 of shapes k each, where k = number of base policies\n",
    "    @Param:\n",
    "    1. policy_dict - (dict[list]) policy matrix of shape (n, n-1) for n agents.\n",
    "    2. N - (int) number of trajectories to run value estimation for.\n",
    "    @Return:\n",
    "    - X: (nd.array) concatenation of X_1 and X_2\n",
    "    \"\"\"\n",
    "    X_1, X_2 = [], [] #store X_1 and X_2 for k policies\n",
    "    K = len(policy_dict.keys())\n",
    "    \n",
    "    for i in range(1, K + 1):\n",
    "        ### compute X_1 with evaluation policy, Ï€_e = Ï€_i\n",
    "        evaluation_agent = get_eval_agent(i)\n",
    "        ### generate set of behavior policies for Ï€_e = Ï€_1, i.e. Ï€_k = {[Ï€_j] for j â‰  i}\n",
    "        behavior_agents = get_behavior_policies(evaluation_agent)\n",
    "        x1 = function(behavior_agents, evaluation_agent, N, 0) #compute x1\n",
    "        x2 = function(behavior_agents, evaluation_agent, N, 1) #compute x2\n",
    "        ### store values\n",
    "        X_1.append(x1)\n",
    "        X_2.append(x2)\n",
    "        \n",
    "    #typecast to nd.array\n",
    "    X_1 = np.array([X_1])\n",
    "    X_2 = np.array([X_2])\n",
    "    ones = np.ones(X_1.shape)\n",
    "    X = np.hstack((X_1.T, X_2.T, ones.T)) #concat\n",
    "    \n",
    "    #Test dimensions\n",
    "    assert(X.shape == (K, 3))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate X with 50 trajectories\n",
    "X = main(policy_dict, 1000)\n",
    "### generate true value estimate\n",
    "true_values = Value(policy_dict, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSE(\"lr\") #set regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.888609052210118e-31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse.mse(X, true_values) # train regression algorithm and compute the mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coef_': array([[-0.07151347, -0.01465456,  0.        ]]),\n",
       " 'intercept_': array([8.52142941])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse.getParams() # get coefficients and y-intercept from running regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.925000000000001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values.mean() ## E[v(Ï€)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
