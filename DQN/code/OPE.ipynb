{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main program on estimating error function for Off-Policy-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Box2D\n",
    "# !pip install 'gym[all]'\n",
    "# !pip install pyyaml\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from dqnetwork import DQNetwork\n",
    "from agent import Agent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os import listdir, getcwd\n",
    "from os.path import isabs, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the environment\n",
    "env_id = 'CartPole-v0'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all model policies - \"../model/policies/behavior_policy_x.pth\"\n",
    "def get_policies(path, config):\n",
    "    \"\"\"\n",
    "    Loads all policies in a given directory into an agent\n",
    "    @Param:\n",
    "    1. Path - model path.\n",
    "    2. config - path for configuration of corresponding model\n",
    "    @Return:\n",
    "    - policies: list of agent\n",
    "    \"\"\"\n",
    "    cwd = getcwd() #get current working directory\n",
    "    index = lambda index : int(re.findall(r'[0-9]+', index)[-1]) #get policy number\n",
    "        \n",
    "    #Get paths for config and policy.\n",
    "    policy_files = {index(f): join(path, f) for f in listdir(join(cwd, path)) if isabs(join(cwd, f))}\n",
    "    yaml_files   = [join(cwd, join(config, f)) for f in listdir(join(cwd, config)) if isabs(join(cwd, f))]\n",
    "\n",
    "    policies = [] #loads an agent policy\n",
    "    \n",
    "    for config_file in yaml_files:\n",
    "        with open(r''+ config_file) as file:\n",
    "            config_data = yaml.load(file, Loader=yaml.FullLoader) #get output as dict\n",
    "            info = config_data[0] #gets model size info\n",
    "            num = index(config_file) #get file number\n",
    "            agent = Agent(fc1=info['fc1'], fc2=info['fc2'], path=policy_files[num])\n",
    "            policies.append( agent ) #add agent\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded into local and target networks!\n",
      "Model loaded into local and target networks!\n",
      "Model loaded into local and target networks!\n"
     ]
    }
   ],
   "source": [
    "agents = get_policies(\"../model/policies\", \"../model/config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - generate policy matrix\n",
    "<p> A policy matrix is a dictionary of dimensions (K, K - 1), where K is the total number of behavior policies </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_matrix(agents):\n",
    "    \"\"\"Generates policy matrix (dictionary) of shape (n, n-1) for x agents\"\"\"\n",
    "    matrix = {}\n",
    "    for i, evaluation in enumerate(agents):\n",
    "        matrix[evaluation] = []\n",
    "        for j, behavior in enumerate(agents):\n",
    "            if(i != j):\n",
    "                matrix[evaluation].append(behavior)\n",
    "    return matrix        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each key indicates evaluation policy, and corresponding values indicate behavior policies\n",
    "policy_dict = policy_matrix(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_behavior_policies(evaluation_policy):\n",
    "    \"\"\"Generates respective behvaior policies for a particular evaluation policy\"\"\"\n",
    "    return policy_dict[evaluation_policy] if evaluation_policy in policy_dict.keys() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Get Horizons\n",
    "\n",
    "$$ \\xi_k =  \\prod_{t=1}^{H} \\frac{ \\pi_e({a_t}^k | {s_t}^k) }{ \\pi_k({a_t}^k | {s_t}^k) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizon(evaluation_agent, behavior_agent):\n",
    "    \"\"\"\n",
    "    > Calculates ratio between an evaluation and a behavior agent for infinite horizon\n",
    "    > Calculates Return value (total reward) for behavior_agent\n",
    "    NOTE: behavior_agent (π_k) generates all states and action.\n",
    "    @Param:\n",
    "    1. evaluation_agent - agent class object representing evaluation policy.\n",
    "    2. behavior_agent - agent class object representing behavior policy.\n",
    "    @Returns:\n",
    "    - ratio : value ratio b/w evaluation and behavior agent.\n",
    "    - total_reward : return of behavior_agent.\n",
    "    \"\"\"\n",
    "    ratio = 1\n",
    "    total_reward = 0\n",
    "    state = env.reset() #reset\n",
    "    while True:\n",
    "        action, prob_behv = behavior_agent.get_action(state, eps=0) #generate best action and prob for behavior.\n",
    "        _, prob_eval = evaluation_agent.get_action(state, eps=0) #generate max probability for evaluation policy.\n",
    "        \n",
    "        ratio *= float(prob_eval/prob_behv) #compute ratio\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action) #transition\n",
    "        \n",
    "        total_reward += reward #update reward\n",
    "        state = next_state #update state\n",
    "        \n",
    "        if(done): #stopping condition\n",
    "            break\n",
    "    \n",
    "    return ratio, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Get Trajectories\n",
    "\n",
    "$$ \\sigma(\\pi_e, \\pi_k) = \\sum\\limits_{i=1}^N {R_k}^i \\times {\\xi_k}^i  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories(evaluation_agent, behavior_agent, N, type):\n",
    "    \"\"\"\n",
    "    See formula above.\n",
    "    Calculates the sum of value for a behavior policy and corresponding evaluation policy\n",
    "    based on N trajectories.\n",
    "    \n",
    "    > Computes the sum of ratio b/w evaluation and behavior agent, 𝜉\n",
    "    > Compute sigma that represents inner sum for 1 behavior policy and corresponding evaluation policy\n",
    "    \n",
    "    @Param:\n",
    "    1. evaluation_agent - (Agent) Evaluation Policy\n",
    "    2. behavior_agent - (Agent) Behavior Policy\n",
    "    3. N - (int) number of trajectories\n",
    "    4. type - (int) 0/1 representing x_1 (0) and x_2 (1) for matrix multiplication later.\n",
    "    @Return \n",
    "    - sigma = (float) returns inner sum, sigma (see formula for more details).\n",
    "    \"\"\"\n",
    "    sigma = 0\n",
    "    for _ in range(N):\n",
    "        reward, xi = horizon(evaluation_agent, behavior_agent)\n",
    "        sigma += reward * xi if(type == 0) else reward #X_2 doesn't use xi because it's distributed later.\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Calculate function value\n",
    "\n",
    "$$ f(\\hat{\\pi}) = \\frac{1}{K N} \\sum\\limits_{k=1}^K \\sigma(\\pi_e, \\pi_k) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(behavior_agents, evaluation_agent, N, type):\n",
    "    \"\"\"\n",
    "    Computes f(π) using formula shown above.\n",
    "    @Param:\n",
    "    1. behavior_agent: (list) list of Agent class objects representing set of evaluation for given π_e.\n",
    "    2. evaluation_agents: π_e (Agent) behavior policy\n",
    "    3. N - (int) number of trajectories (used in calculation of sigma in `get_trajectories`)\n",
    "    4. type: (int) 0/1 for calculation of X_1/X_2 with regards sum of products w.r.t constants, c and d.\n",
    "    @Return\n",
    "    - value: (float) value of function f(π) using formula above.\n",
    "    \"\"\"\n",
    "    value = 0\n",
    "    K = len(behavior_agents)\n",
    "    for agent in behavior_agents:\n",
    "        sigma = get_trajectories(evaluation_agent, agent, N, type)\n",
    "        value += sigma\n",
    "    \n",
    "    return float(value / (K * N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. True Value function for an evaluation agent\n",
    "\n",
    "$$ V(\\pi_e) = \\frac{1}{N} \\sum\\limits_{i=1}^N {R_e}^i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value(policy_dict, N):\n",
    "    \"\"\"\n",
    "    Calculates the expected return for all evaluation agents\n",
    "    @Param: \n",
    "    1. policy_dict - (dict[list]) policy matrix of shape (n, n-1) for n agents.\n",
    "    2. N - (int) number of trajectories to run value estimation for.\n",
    "    NOTE: N should equal with estimation value function parameter N.\n",
    "    @Return:\n",
    "    - values - (nd.array) Vector of values of evaluation policies using formula shown above. \n",
    "    \"\"\"\n",
    "    evaluation_agents = list(policy_dict.keys()) #generate n eval agents\n",
    "    values = [0]*len(evaluation_agents) #vector of values of evaluation policies\n",
    "    \n",
    "    for i, agent in enumerate(evaluation_agents):\n",
    "        value = [] #stores N Return for agent with policy π_e.\n",
    "        for n in range(N):\n",
    "            total_reward = 0\n",
    "            state = env.reset() #reset\n",
    "            while True:\n",
    "                action, prob = agent.get_action(state, eps=0)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if(done): break\n",
    "                    \n",
    "            #append to value\n",
    "            value.append(total_reward)\n",
    "        #compute mean return and store in vector of values.\n",
    "        expected_val = np.mean(value)\n",
    "        values[i] = expected_val\n",
    "    \n",
    "    return np.array([values]).T #dim = number of agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Tying it al together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve an evaluation policy with agent of index i.\n",
    "get_eval_agent = lambda i: list(policy_dict.keys())[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(policy_dict, N):\n",
    "    \"\"\"\n",
    "    Main function for generating X_1 and X_2 of shapes k each, where k = number of base policies\n",
    "    @Param:\n",
    "    1. policy_dict - (dict[list]) policy matrix of shape (n, n-1) for n agents.\n",
    "    2. N - (int) number of trajectories to run value estimation for.\n",
    "    @Return:\n",
    "    - X: (nd.array) concatenation of X_1 and X_2\n",
    "    \"\"\"\n",
    "    X_1, X_2 = [], [] #store X_1 and X_2 for k policies\n",
    "    K = len(policy_dict.keys())\n",
    "    \n",
    "    for i in range(1, K + 1):\n",
    "        ### compute X_1 with evaluation policy, π_e = π_i\n",
    "        evaluation_agent = get_eval_agent(i)\n",
    "        ### generate set of behavior policies for π_e = π_1, i.e. π_k = {[π_j] for j ≠ i}\n",
    "        behavior_agents = get_behavior_policies(evaluation_agent)\n",
    "        x1 = function(behavior_agents, evaluation_agent, N, 0) #compute x1\n",
    "        x2 = function(behavior_agents, evaluation_agent, N, 1) #compute x2\n",
    "        ### store values\n",
    "        X_1.append(x1)\n",
    "        X_2.append(x2)\n",
    "        \n",
    "    #typecast to nd.array\n",
    "    X_1 = np.array([X_1])\n",
    "    X_2 = np.array([X_2])\n",
    "    X = np.hstack((X_1.T, X_2.T)) #concat\n",
    "    \n",
    "    #Test dimensions\n",
    "    assert(X.shape == (K, 2))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate X with 50 trajectories\n",
    "X = main(policy_dict, 50)\n",
    "### generate true value estimate\n",
    "true_values = Value(policy_dict, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  93028.98539208976\n"
     ]
    }
   ],
   "source": [
    "c, d = 2e-21, 0\n",
    "Y_intercept = np.array([[c, d]]).T\n",
    "\n",
    "output = np.dot(X, Y_intercept)\n",
    "error = np.sum(np.square(output - true_values))\n",
    "print(\"Error = \", error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
