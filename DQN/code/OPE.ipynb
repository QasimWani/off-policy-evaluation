{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main program on estimating error function for Off-Policy-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Box2D\n",
    "# !pip install 'gym[all]'\n",
    "# !pip install pyyaml\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from dqnetwork import DQNetwork\n",
    "from agent import Agent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os import listdir, getcwd\n",
    "from os.path import isabs, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the environment\n",
    "env_id = 'CartPole-v0'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all model policies - \"../model/policies/behavior_policy_x.pth\"\n",
    "def get_policies(path, config):\n",
    "    \"\"\"\n",
    "    Loads all policies in a given directory into an agent\n",
    "    @Param:\n",
    "    1. Path - model path.\n",
    "    2. config - path for configuration of corresponding model\n",
    "    @Return:\n",
    "    - policies: list of agent\n",
    "    \"\"\"\n",
    "    cwd = getcwd() #get current working directory\n",
    "    index = lambda index : int(re.findall(r'[0-9]+', index)[-1]) #get policy number\n",
    "        \n",
    "    #Get paths for config and policy.\n",
    "    policy_files = {index(f): join(path, f) for f in listdir(join(cwd, path)) if isabs(join(cwd, f))}\n",
    "    yaml_files   = [join(cwd, join(config, f)) for f in listdir(join(cwd, config)) if isabs(join(cwd, f))]\n",
    "\n",
    "    policies = [] #loads an agent policy\n",
    "    \n",
    "    for config_file in yaml_files:\n",
    "        with open(r''+ config_file) as file:\n",
    "            config_data = yaml.load(file, Loader=yaml.FullLoader) #get output as dict\n",
    "            info = config_data[0] #gets model size info\n",
    "            num = index(config_file) #get file number\n",
    "            agent = Agent(fc1=info['fc1'], fc2=info['fc2'], path=policy_files[num])\n",
    "            policies.append( agent ) #add agent\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded into local and target networks!\n",
      "Model loaded into local and target networks!\n",
      "Model loaded into local and target networks!\n"
     ]
    }
   ],
   "source": [
    "agents = get_policies(\"../model/policies\", \"../model/config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - generate policy matrix\n",
    "<p> A policy matrix is a dictionary of dimensions (K, K - 1), where K is the total number of behavior policies </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_matrix(agents):\n",
    "    \"\"\"Generates policy matrix (dictionary) of shape (n, n-1) for x agents\"\"\"\n",
    "    matrix = {}\n",
    "    for i, evaluation in enumerate(agents):\n",
    "        matrix[evaluation] = []\n",
    "        for j, behavior in enumerate(agents):\n",
    "            if(i != j):\n",
    "                matrix[evaluation].append(behavior)\n",
    "    return matrix        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each key indicates evaluation policy, and corresponding values indicate behavior policies\n",
    "policy_dict = policy_matrix(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_behavior_policies(evaluation_policy):\n",
    "    \"\"\"Generates respective behvaior policies for a particular evaluation policy\"\"\"\n",
    "    return policy_dict[evaluation_policy] if evaluation_policy in policy_dict else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Get Horizons\n",
    "\n",
    "$$ \\xi_k =  \\prod_{t=1}^{H} \\frac{ \\pi_e({a_t}^k | {s_t}^k) }{ \\pi_k({a_t}^k | {s_t}^k) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizon(evaluation_agent, behavior_agent):\n",
    "    \"\"\"\n",
    "    > Calculates ratio between an evaluation and a behavior agent for infinite horizon\n",
    "    > Calculates Return value (total reward) for behavior_agent\n",
    "    NOTE: behavior_agent (π_k) generates all states and action.\n",
    "    @Param:\n",
    "    1. evaluation_agent - agent class object representing evaluation policy.\n",
    "    2. behavior_agent - agent class object representing behavior policy.\n",
    "    @Returns:\n",
    "    - ratio : value ratio b/w evaluation and behavior agent.\n",
    "    - total_reward : return of behavior_agent.\n",
    "    \"\"\"\n",
    "    ratio = 1\n",
    "    total_reward = 0\n",
    "    state = env.reset() #reset\n",
    "    while True:\n",
    "        action, prob_behv = behavior_agent.get_action(state, eps=0) #generate best action and prob for behavior.\n",
    "        _, prob_eval = evaluation_agent.get_action(state, eps=0) #generate max probability for evaluation policy.\n",
    "        \n",
    "        ratio *= float(prob_eval/prob_behv) #compute ratio\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action) #transition\n",
    "        \n",
    "        total_reward += reward #update reward\n",
    "        state = next_state #update state\n",
    "        \n",
    "        if(done): #stopping condition\n",
    "            break\n",
    "    \n",
    "    return ratio, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Get Trajectories\n",
    "\n",
    "$$ \\sigma = \\sum\\limits_{i=1}^N {R_k}^i \\times \\xi_k  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories():\n",
    "    \"\"\"\n",
    "    See formula above.\n",
    "    > Computes the sum of ratio b/w evaluation and behavior agent, 𝜉\n",
    "    > Compute sigma that represents \n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    rewards = []\n",
    "    \n",
    "    for n in range(N):\n",
    "        transitions = []\n",
    "        total_reward = 0\n",
    "        state = env.reset() #reset\n",
    "        while True:\n",
    "            action, prob = agent.get_action(state, eps=0)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            transitions.append((state, action, reward, next_state)) #store transition\n",
    "            state = next_state\n",
    "            if(render):\n",
    "                env.render() #display agent\n",
    "            if(done): #stopping condition\n",
    "                trajectories.append(transitions)\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "    \n",
    "    assert(np.mean(rewards) >= 195) #condition for expert policy\n",
    "    \n",
    "    return np.array(trajectories), np.array(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
